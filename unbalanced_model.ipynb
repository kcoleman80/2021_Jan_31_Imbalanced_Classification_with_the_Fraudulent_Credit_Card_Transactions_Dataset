{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/imbalanced-classification-of-good-and-bad-credit/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/jbrownlee/Datasets/master/german.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/german.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 21)\n",
      "Class=1, Count=700, Percentage=70.000%\n",
      "Class=2, Count=300, Percentage=30.000%\n"
     ]
    }
   ],
   "source": [
    "# load and summarize the dataset\n",
    "from pandas import read_csv\n",
    "from collections import Counter\n",
    "# define the dataset location\n",
    "filename = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/german.csv'\n",
    "# load the csv file as a data frame\n",
    "dataframe = read_csv(filename, header=None)\n",
    "# summarize the shape of the dataset\n",
    "print(dataframe.shape)\n",
    "# summarize the class distribution\n",
    "target = dataframe.values[:,-1]\n",
    "counter = Counter(target)\n",
    "for k,v in counter.items():\n",
    "\tper = v / len(target) * 100\n",
    "\tprint('Class=%d, Count=%d, Percentage=%.3f%%' % (k, v, per))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create histograms of numeric input variables\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "# define the dataset location\n",
    "filename = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/german.csv'\n",
    "# load the csv file as a data frame\n",
    "df = read_csv(filename, header=None)\n",
    "# select columns with numerical data types\n",
    "num_ix = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "# select a subset of the dataframe with the chosen columns\n",
    "subset = df[num_ix]\n",
    "# create a histogram plot of each numeric variable\n",
    "ax = subset.hist()\n",
    "# disable axis labels to avoid the clutter\n",
    "for axis in ax.flatten():\n",
    "\taxis.set_xticklabels([])\n",
    "\taxis.set_yticklabels([])\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Test and Baseline Result\n",
    "\n",
    "We will evaluate candidate models using repeated stratified k-fold cross-validation.\n",
    "\n",
    "The k-fold cross-validation procedure provides a good general estimate of model performance that is not too optimistically biased, at least compared to a single train-test split. We will use k=10, meaning each fold will contain about 1000/10 or 100 examples.\n",
    "\n",
    "Stratified means that each fold will contain the same mixture of examples by class, that is about 70 percent to 30 percent good to bad customers. Repeated means that the evaluation process will be performed multiple times to help avoid fluke results and better capture the variance of the chosen model. We will use three repeats.\n",
    "\n",
    "This means a single model will be fit and evaluated 10 * 3 or 30 times and the mean and standard deviation of these runs will be reported.\n",
    "\n",
    "This can be achieved using the RepeatedStratifiedKFold scikit-learn class.\n",
    "\n",
    "We will predict class labels of whether a customer is good or not. Therefore, we need a measure that is appropriate for evaluating the predicted class labels.\n",
    "\n",
    "The focus of the task is on the positive class (bad customers). Precision and recall are a good place to start. Maximizing precision will minimize the false positives and maximizing recall will minimize the false negatives in the predictions made by a model.\n",
    "\n",
    "Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "Using the F-Measure will calculate the harmonic mean between precision and recall. This is a good single number that can be used to compare and select a model on this problem. The issue is that false negatives are more damaging than false positives.\n",
    "\n",
    "F-Measure = (2 * Precision * Recall) / (Precision + Recall)\n",
    "Remember that false negatives on this dataset are cases of a bad customer being marked as a good customer and being given a loan. False positives are cases of a good customer being marked as a bad customer and not being given a loan.\n",
    "\n",
    "False Negative: Bad Customer (class 1) predicted as a Good Customer (class 0).\n",
    "False Positive: Good Customer (class 0) predicted as a Bad Customer (class 1).\n",
    "False negatives are more costly to the bank than false positives.\n",
    "\n",
    "Cost(False Negatives) > Cost(False Positives)\n",
    "Put another way, we are interested in the F-measure that will summarize a model’s ability to minimize misclassification errors for the positive class, but we want to favor models that are better are minimizing false negatives over false positives.\n",
    "\n",
    "This can be achieved by using a version of the F-measure that calculates a weighted harmonic mean of precision and recall but favors higher recall scores over precision scores. This is called the Fbeta-measure, a generalization of F-measure, where “beta” is a parameter that defines the weighting of the two scores.\n",
    "\n",
    "Fbeta-Measure = ((1 + beta^2) * Precision * Recall) / (beta^2 * Precision + Recall)\n",
    "A beta value of 2 will weight more attention on recall than precision and is referred to as the F2-measure.\n",
    "\n",
    "F2-Measure = ((1 + 2^2) * Precision * Recall) / (2^2 * Precision + Recall)\n",
    "We will use this measure to evaluate models on the German credit dataset. This can be achieved using the fbeta_score() scikit-learn function.\n",
    "\n",
    "We can define a function to load the dataset and split the columns into input and output variables. We will one-hot encode the categorical variables and label encode the target variable. You might recall that a one-hot encoding replaces the categorical variable with one new column for each value of the variable and marks values with a 1 in the column for that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 61) (1000,) Counter({0: 700, 1: 300})\n",
      "Mean F2: 0.682 (0.000)\n"
     ]
    }
   ],
   "source": [
    "# test harness and baseline model evaluation for the german credit dataset\n",
    "from collections import Counter\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "\t# load the dataset as a numpy array\n",
    "\tdataframe = read_csv(full_path, header=None)\n",
    "\t# split into inputs and outputs\n",
    "\tlast_ix = len(dataframe.columns) - 1\n",
    "\tX, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "\t# select categorical features\n",
    "\tcat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "\t# one hot encode cat features only\n",
    "\tct = ColumnTransformer([('o',OneHotEncoder(),cat_ix)], remainder='passthrough')\n",
    "\tX = ct.fit_transform(X)\n",
    "\t# label encode the target variable to have the classes 0 and 1\n",
    "\ty = LabelEncoder().fit_transform(y)\n",
    "\treturn X, y\n",
    "\n",
    "# calculate f2 score\n",
    "def f2(y_true, y_pred):\n",
    "\treturn fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# define the model evaluation metric\n",
    "\tmetric = make_scorer(f2)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define the location of the dataset\n",
    "full_path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/german.csv'\n",
    "# load the dataset\n",
    "X, y = load_dataset(full_path)\n",
    "# summarize the loaded dataset\n",
    "print(X.shape, y.shape, Counter(y))\n",
    "# define the reference model\n",
    "model = DummyClassifier(strategy='constant', constant=1)\n",
    "# evaluate the model\n",
    "scores = evaluate_model(X, y, model)\n",
    "# summarize performance\n",
    "print('Mean F2: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Models\n",
    "\n",
    "In this section, we will evaluate a suite of different techniques on the dataset using the test harness developed in the previous section.\n",
    "\n",
    "The goal is to both demonstrate how to work through the problem systematically and to demonstrate the capability of some techniques designed for imbalanced classification problems.\n",
    "\n",
    "The reported performance is good, but not highly optimized (e.g. hyperparameters are not tuned).\n",
    "\n",
    "Can you do better? If you can achieve better F2-Measure performance using the same test harness, I’d love to hear about it. Let me know in the comments below.\n",
    "\n",
    "Evaluate Machine Learning Algorithms\n",
    "\n",
    "Let’s start by evaluating a mixture of probabilistic machine learning models on the dataset.\n",
    "\n",
    "It can be a good idea to spot check a suite of different linear and nonlinear algorithms on a dataset to quickly flush out what works well and deserves further attention, and what doesn’t.\n",
    "\n",
    "We will evaluate the following machine learning models on the German credit dataset:\n",
    "\n",
    "Logistic Regression (LR)\n",
    "Linear Discriminant Analysis (LDA)\n",
    "Naive Bayes (NB)\n",
    "Gaussian Process Classifier (GPC)\n",
    "Support Vector Machine (SVM)\n",
    "We will use mostly default model hyperparameters.\n",
    "\n",
    "We will define each model in turn and add them to a list so that we can evaluate them sequentially. The get_models() function below defines the list of models for evaluation, as well as a list of model short names for plotting the results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LR 0.498 (0.072)\n",
      ">LDA 0.519 (0.072)\n",
      ">NB 0.639 (0.049)\n",
      ">GPC 0.219 (0.061)\n",
      ">SVM 0.436 (0.077)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spot check machine learning algorithms on the german credit dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "\t# load the dataset as a numpy array\n",
    "\tdataframe = read_csv(full_path, header=None)\n",
    "\t# split into inputs and outputs\n",
    "\tlast_ix = len(dataframe.columns) - 1\n",
    "\tX, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "\t# select categorical and numerical features\n",
    "\tcat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "\tnum_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\t# label encode the target variable to have the classes 0 and 1\n",
    "\ty = LabelEncoder().fit_transform(y)\n",
    "\treturn X.values, y, cat_ix, num_ix\n",
    "\n",
    "# calculate f2-measure\n",
    "def f2_measure(y_true, y_pred):\n",
    "\treturn fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# define the model evaluation metric\n",
    "\tmetric = make_scorer(f2_measure)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define models to test\n",
    "def get_models():\n",
    "\tmodels, names = list(), list()\n",
    "\t# LR\n",
    "\tmodels.append(LogisticRegression(solver='liblinear'))\n",
    "\tnames.append('LR')\n",
    "\t# LDA\n",
    "\tmodels.append(LinearDiscriminantAnalysis())\n",
    "\tnames.append('LDA')\n",
    "\t# NB\n",
    "\tmodels.append(GaussianNB())\n",
    "\tnames.append('NB')\n",
    "\t# GPC\n",
    "\tmodels.append(GaussianProcessClassifier())\n",
    "\tnames.append('GPC')\n",
    "\t# SVM\n",
    "\tmodels.append(SVC(gamma='scale'))\n",
    "\tnames.append('SVM')\n",
    "\treturn models, names\n",
    "\n",
    "# define the location of the dataset\n",
    "full_path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define models\n",
    "models, names = get_models()\n",
    "results = list()\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# one hot encode categorical, normalize numerical\n",
    "\tct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "\t# wrap the model i a pipeline\n",
    "\tpipeline = Pipeline(steps=[('t',ct),('m',models[i])])\n",
    "\t# evaluate the model and store results\n",
    "\tscores = evaluate_model(X, y, pipeline)\n",
    "\tresults.append(scores)\n",
    "\t# summarize and store\n",
    "\tprint('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n",
    "# plot the results\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Undersampling\n",
    "\n",
    "Undersampling is perhaps the least widely used technique when addressing an imbalanced classification task as most of the focus is put on oversampling the majority class with SMOTE.\n",
    "\n",
    "Undersampling can help to remove examples from the majority class along the decision boundary that make the problem challenging for classification algorithms.\n",
    "\n",
    "In this experiment we will test the following undersampling algorithms:\n",
    "\n",
    "Tomek Links (TL)\n",
    "Edited Nearest Neighbors (ENN)\n",
    "Repeated Edited Nearest Neighbors (RENN)\n",
    "One Sided Selection (OSS)\n",
    "Neighborhood Cleaning Rule (NCR)\n",
    "The Tomek Links and ENN methods select examples from the majority class to delete, whereas OSS and NCR both select examples to keep and examples to delete. We will use the balanced version of the logistic regression algorithm to test each undersampling method, to keep things simple.\n",
    "\n",
    "The get_models() function from the previous section can be updated to return a list of undersampling techniques to test with the logistic regression algorithm. We use the implementations of these algorithms from the imbalanced-learn library.\n",
    "\n",
    "The updated version of the get_models() function defining the undersampling methods is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">TL 0.669 (0.057)\n",
      ">ENN 0.706 (0.048)\n",
      ">RENN 0.714 (0.041)\n",
      ">OSS 0.668 (0.058)\n",
      ">NCR 0.693 (0.052)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate undersampling with logistic regression on the imbalanced german credit dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from matplotlib import pyplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "\t# load the dataset as a numpy array\n",
    "\tdataframe = read_csv(full_path, header=None)\n",
    "\t# split into inputs and outputs\n",
    "\tlast_ix = len(dataframe.columns) - 1\n",
    "\tX, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "\t# select categorical and numerical features\n",
    "\tcat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "\tnum_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\t# label encode the target variable to have the classes 0 and 1\n",
    "\ty = LabelEncoder().fit_transform(y)\n",
    "\treturn X.values, y, cat_ix, num_ix\n",
    "\n",
    "# calculate f2-measure\n",
    "def f2_measure(y_true, y_pred):\n",
    "\treturn fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# define the model evaluation metric\n",
    "\tmetric = make_scorer(f2_measure)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define undersampling models to test\n",
    "def get_models():\n",
    "\tmodels, names = list(), list()\n",
    "\t# TL\n",
    "\tmodels.append(TomekLinks())\n",
    "\tnames.append('TL')\n",
    "\t# ENN\n",
    "\tmodels.append(EditedNearestNeighbours())\n",
    "\tnames.append('ENN')\n",
    "\t# RENN\n",
    "\tmodels.append(RepeatedEditedNearestNeighbours())\n",
    "\tnames.append('RENN')\n",
    "\t# OSS\n",
    "\tmodels.append(OneSidedSelection())\n",
    "\tnames.append('OSS')\n",
    "\t# NCR\n",
    "\tmodels.append(NeighbourhoodCleaningRule())\n",
    "\tnames.append('NCR')\n",
    "\treturn models, names\n",
    "\n",
    "# define the location of the dataset\n",
    "full_path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define models\n",
    "models, names = get_models()\n",
    "results = list()\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# define model to evaluate\n",
    "\tmodel = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "\t# one hot encode categorical, normalize numerical\n",
    "\tct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "\t# scale, then undersample, then fit model\n",
    "\tpipeline = Pipeline(steps=[('t',ct), ('s', models[i]), ('m',model)])\n",
    "\t# evaluate the model and store results\n",
    "\tscores = evaluate_model(X, y, pipeline)\n",
    "\tresults.append(scores)\n",
    "\t# summarize and store\n",
    "\tprint('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n",
    "# plot the results\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Model Improvements\n",
    "\n",
    "This is a new section that provides a minor departure to the above section. Here, we will test specific models that result in a further lift in F2-measure performance and I will update this section as new models are reported/discovered.\n",
    "\n",
    "Improvement #1: InstanceHardnessThreshold\n",
    "\n",
    "An F2-measure of about 0.727 can be achieved using balanced Logistic Regression with InstanceHardnessThreshold undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724 (0.034)\n"
     ]
    }
   ],
   "source": [
    "# improve performance on the imbalanced german credit dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "\t# load the dataset as a numpy array\n",
    "\tdataframe = read_csv(full_path, header=None)\n",
    "\t# split into inputs and outputs\n",
    "\tlast_ix = len(dataframe.columns) - 1\n",
    "\tX, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "\t# select categorical and numerical features\n",
    "\tcat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "\tnum_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\t# label encode the target variable to have the classes 0 and 1\n",
    "\ty = LabelEncoder().fit_transform(y)\n",
    "\treturn X.values, y, cat_ix, num_ix\n",
    "\n",
    "# calculate f2-measure\n",
    "def f2_measure(y_true, y_pred):\n",
    "\treturn fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# define the model evaluation metric\n",
    "\tmetric = make_scorer(f2_measure)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define the location of the dataset\n",
    "full_path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define model to evaluate\n",
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "# define the data sampling\n",
    "sampling = InstanceHardnessThreshold()\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "# scale, then sample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', sampling), ('m',model)])\n",
    "# evaluate the model and store results\n",
    "scores = evaluate_model(X, y, pipeline)\n",
    "print('%.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement #2: SMOTEENN\n",
    "\n",
    "An F2-measure of about 0.730 can be achieved using LDA with SMOTEENN, where the ENN parameter is set to an ENN instance with sampling_strategy set to majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728 (0.045)\n"
     ]
    }
   ],
   "source": [
    "# improve performance on the imbalanced german credit dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "\t# load the dataset as a numpy array\n",
    "\tdataframe = read_csv(full_path, header=None)\n",
    "\t# split into inputs and outputs\n",
    "\tlast_ix = len(dataframe.columns) - 1\n",
    "\tX, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "\t# select categorical and numerical features\n",
    "\tcat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "\tnum_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\t# label encode the target variable to have the classes 0 and 1\n",
    "\ty = LabelEncoder().fit_transform(y)\n",
    "\treturn X.values, y, cat_ix, num_ix\n",
    "\n",
    "# calculate f2-measure\n",
    "def f2_measure(y_true, y_pred):\n",
    "\treturn fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# define the model evaluation metric\n",
    "\tmetric = make_scorer(f2_measure)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define the location of the dataset\n",
    "full_path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define model to evaluate\n",
    "model = LinearDiscriminantAnalysis()\n",
    "# define the data sampling\n",
    "sampling = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "# scale, then sample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', sampling), ('m',model)])\n",
    "# evaluate the model and store results\n",
    "scores = evaluate_model(X, y, pipeline)\n",
    "print('%.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement #3: SMOTEENN with StandardScaler and RidgeClassifier\n",
    "\n",
    "An F2-measure of about 0.741 can be achieved with further improvements to the SMOTEENN using a RidgeClassifier instead of LDA and using a StandardScaler for the numeric inputs instead of a MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.735 (0.040)\n"
     ]
    }
   ],
   "source": [
    "# improve performance on the imbalanced german credit dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "\t# load the dataset as a numpy array\n",
    "\tdataframe = read_csv(full_path, header=None)\n",
    "\t# split into inputs and outputs\n",
    "\tlast_ix = len(dataframe.columns) - 1\n",
    "\tX, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "\t# select categorical and numerical features\n",
    "\tcat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "\tnum_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\t# label encode the target variable to have the classes 0 and 1\n",
    "\ty = LabelEncoder().fit_transform(y)\n",
    "\treturn X.values, y, cat_ix, num_ix\n",
    "\n",
    "# calculate f2-measure\n",
    "def f2_measure(y_true, y_pred):\n",
    "\treturn fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# define the model evaluation metric\n",
    "\tmetric = make_scorer(f2_measure)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define the location of the dataset\n",
    "full_path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define model to evaluate\n",
    "model = RidgeClassifier()\n",
    "# define the data sampling\n",
    "sampling = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',StandardScaler(),num_ix)])\n",
    "# scale, then sample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', sampling), ('m',model)])\n",
    "# evaluate the model and store results\n",
    "scores = evaluate_model(X, y, pipeline)\n",
    "print('%.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Prediction on New Data\n",
    "\n",
    "Given the variance in results, a selection of any of the undersampling methods is probably sufficient. In this case, we will select logistic regression with Repeated ENN.\n",
    "\n",
    "This model had an F2-measure of about about 0.716 on our test harness.\n",
    "\n",
    "We will use this as our final model and use it to make predictions on new data.\n",
    "\n",
    "First, we can define the model as a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Customers:\n",
      ">Predicted=0 (expected 0)\n",
      ">Predicted=0 (expected 0)\n",
      ">Predicted=1 (expected 0)\n",
      "Bad Customers:\n",
      ">Predicted=1 (expected 1)\n",
      ">Predicted=1 (expected 1)\n",
      ">Predicted=1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "# fit a model and make predictions for the german credit dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "\t# load the dataset as a numpy array\n",
    "\tdataframe = read_csv(full_path, header=None)\n",
    "\t# split into inputs and outputs\n",
    "\tlast_ix = len(dataframe.columns) - 1\n",
    "\tX, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "\t# select categorical and numerical features\n",
    "\tcat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "\tnum_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\t# label encode the target variable to have the classes 0 and 1\n",
    "\ty = LabelEncoder().fit_transform(y)\n",
    "\treturn X.values, y, cat_ix, num_ix\n",
    "\n",
    "# define the location of the dataset\n",
    "full_path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define model to evaluate\n",
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "# scale, then undersample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', RepeatedEditedNearestNeighbours()), ('m',model)])\n",
    "# fit the model\n",
    "pipeline.fit(X, y)\n",
    "# evaluate on some good customers cases (known class 0)\n",
    "print('Good Customers:')\n",
    "data = [['A11', 6, 'A34', 'A43', 1169, 'A65', 'A75', 4, 'A93', 'A101', 4, 'A121', 67, 'A143', 'A152', 2, 'A173', 1, 'A192', 'A201'],\n",
    "\t['A14', 12, 'A34', 'A46', 2096, 'A61', 'A74', 2, 'A93', 'A101', 3, 'A121', 49, 'A143', 'A152', 1, 'A172', 2, 'A191', 'A201'],\n",
    "\t['A11', 42, 'A32', 'A42', 7882, 'A61', 'A74', 2, 'A93', 'A103', 4, 'A122', 45, 'A143', 'A153', 1, 'A173', 2, 'A191', 'A201']]\n",
    "for row in data:\n",
    "\t# make prediction\n",
    "\tyhat = pipeline.predict([row])\n",
    "\t# get the label\n",
    "\tlabel = yhat[0]\n",
    "\t# summarize\n",
    "\tprint('>Predicted=%d (expected 0)' % (label))\n",
    "# evaluate on some bad customers (known class 1)\n",
    "print('Bad Customers:')\n",
    "data = [['A13', 18, 'A32', 'A43', 2100, 'A61', 'A73', 4, 'A93', 'A102', 2, 'A121', 37, 'A142', 'A152', 1, 'A173', 1, 'A191', 'A201'],\n",
    "\t['A11', 24, 'A33', 'A40', 4870, 'A61', 'A73', 3, 'A93', 'A101', 4, 'A124', 53, 'A143', 'A153', 2, 'A173', 2, 'A191', 'A201'],\n",
    "\t['A11', 24, 'A32', 'A43', 1282, 'A62', 'A73', 4, 'A92', 'A101', 2, 'A123', 32, 'A143', 'A152', 1, 'A172', 1, 'A191', 'A201']]\n",
    "for row in data:\n",
    "\t# make prediction\n",
    "\tyhat = pipeline.predict([row])\n",
    "\t# get the label\n",
    "\tlabel = yhat[0]\n",
    "\t# summarize\n",
    "\tprint('>Predicted=%d (expected 1)' % (label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
